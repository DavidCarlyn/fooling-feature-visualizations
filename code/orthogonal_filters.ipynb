{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023 Roland S. Zimmermann\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fooling Feature Visualizations Through Orthogonal Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73cd27-072a-40bd-b80e-07d74407cd03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Union\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Attention: This requires a specific version of Lucid\n",
    "# (i.e., an still actively-developed fork): https://github.com/zimmerrol/lucent\n",
    "import lucent.modelzoo\n",
    "import lucent.modelzoo.util\n",
    "from lucent.optvis.hooks import ModelHook\n",
    "from lucent.optvis import render\n",
    "from lucent.optvis import param\n",
    "\n",
    "# Use FFCV for quick data loading.\n",
    "from pathlib import Path\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToTensor, ToDevice, Squeeze, NormalizeImage, ToTorchImage\n",
    "from ffcv.fields.rgb_image import CenterCropRGBImageDecoder\n",
    "from ffcv.fields.basics import IntDecoder\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c155eff-943d-4d10-84df-eff3c6c3e34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2be3b-257f-46aa-ade6-73ccf7f6d874",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6c262-22e0-4c72-8f25-c91d1b376bab",
   "metadata": {},
   "source": [
    "## Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32838424-219b-4712-8cbe-a3cd12d492c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleneckFooling(nn.Module):\n",
    "    \"\"\"Bottleneck block of the ResNet architecture with fooling units.\"\"\"\n",
    "\n",
    "    def __init__(self, base: resnet.Bottleneck) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = duplicate_conv(base.conv1, bias=False)\n",
    "        self.bn1 = duplicate_norm(base.bn1)\n",
    "        self.conv2 = duplicate_conv(base.conv2, bias=False)\n",
    "        self.bn2 = duplicate_norm(base.bn2)\n",
    "        self.conv3 = duplicate_conv(base.conv3, bias=False)\n",
    "        self.bn3 = duplicate_norm(base.bn3)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.relu1_fooling = nn.ReLU(inplace=False)\n",
    "        self.relu2_fooling = nn.ReLU(inplace=False)\n",
    "        self.relu3_fooling = nn.ReLU(inplace=False)\n",
    "        self.conv1_fooling = duplicate_conv(self.conv1, bias=True)\n",
    "        self.conv2_fooling = duplicate_conv(self.conv2, bias=True)\n",
    "        self.conv3_fooling = duplicate_conv(self.conv3, bias=True)\n",
    "\n",
    "        self.conv1_fooling_merge = identity_conv(self.conv1)\n",
    "        self.conv2_fooling_merge = identity_conv(self.conv2)\n",
    "        self.conv3_fooling_merge = identity_conv(self.conv3)\n",
    "\n",
    "        self.downsample = base.downsample\n",
    "\n",
    "        self.identity = Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.identity(x)\n",
    "\n",
    "        out_fooling = self.conv1_fooling(x)\n",
    "        out_fooling = self.relu1_fooling(out_fooling)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv1_fooling_merge(torch.cat((out, out_fooling), 1))\n",
    "\n",
    "        out_fooling = self.conv2_fooling(out)\n",
    "        out_fooling = self.relu2_fooling(out_fooling)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2_fooling_merge(torch.cat((out, out_fooling), 1))\n",
    "\n",
    "        out_fooling = self.conv3_fooling(out)\n",
    "        out_fooling = self.relu3_fooling(out_fooling)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.conv3_fooling_merge(torch.cat((out, out_fooling), 1))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def duplicate_conv(module: nn.Module, bias: bool = True, overwrite_values: Dict = {}):\n",
    "    \"\"\"Duplicate a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        module: The convolutional layer to duplicate.\n",
    "        bias: Whether to duplicate the bias.\n",
    "        overwrite_values: Values to overwrite in the new layer.\n",
    "\n",
    "    Returns:\n",
    "        The duplicated layer.\n",
    "    \"\"\"\n",
    "    kw = (\n",
    "        \"in_channels\",\n",
    "        \"out_channels\",\n",
    "        \"kernel_size\",\n",
    "        \"stride\",\n",
    "        \"padding\",\n",
    "        \"dilation\",\n",
    "        \"groups\",\n",
    "        \"padding_mode\",\n",
    "    )\n",
    "    kwargs = {k: getattr(module, k) for k in kw}\n",
    "    kwargs = {**kwargs, **overwrite_values}\n",
    "    return type(module)(**kwargs, bias=bias)\n",
    "\n",
    "\n",
    "def duplicate_norm(module: nn.Module):\n",
    "    \"\"\"Duplicate a normalization layer.\n",
    "\n",
    "    Args:\n",
    "        module: The normalization layer to duplicate.\n",
    "\n",
    "    Returns:\n",
    "        The duplicated layer.\n",
    "    \"\"\"\n",
    "    kw = (\"num_features\", \"eps\", \"momentum\")\n",
    "    return type(module)(**{k: getattr(module, k) for k in kw})\n",
    "\n",
    "\n",
    "def identity_conv(previous_conv: nn.Module) -> nn.Module:\n",
    "    \"\"\"Create a convolutional layer that acts as the identity function.\n",
    "\n",
    "    Args:\n",
    "        previous_conv: The convolutional layer to create an identity layer\n",
    "        for (i.e., the previous layer).\n",
    "    \"\"\"\n",
    "    n = previous_conv.weight.data.shape[0]\n",
    "    m = nn.Conv2d(2 * n, n, 3, padding=1, bias=False)\n",
    "    m.weight.data.zero_()\n",
    "    m.weight.data[np.arange(n), np.arange(n), 1, 1] = 1.0\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def replace_module(\n",
    "    model: nn.Module,\n",
    "    check_fn: Callable[[nn.Module], bool],\n",
    "    get_replacement_fn: Callable[[nn.Module], nn.Module],\n",
    ") -> None:\n",
    "    \"\"\"Replace all modules in a model that satisfy a condition.\n",
    "\n",
    "    Args:\n",
    "        model: The model to replace modules in.\n",
    "        check_fn: A function that takes a module and returns whether it should\n",
    "        be replaced.\n",
    "        get_replacement_fn: A function that takes a module and returns a\n",
    "        replacement module.\n",
    "    \"\"\"\n",
    "    children = list(model.named_children())\n",
    "    for name, value in children:\n",
    "        if check_fn(value):\n",
    "            new_value = get_replacement_fn(value)\n",
    "            setattr(model, name, new_value)\n",
    "        replace_module(value, check_fn, get_replacement_fn)\n",
    "\n",
    "\n",
    "def reset_fooling_units(model: nn.Module) -> None:\n",
    "    \"\"\"Reset the weights of the fooling units in a model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to reset the fooling units of.\n",
    "    \"\"\"\n",
    "    for value in model.modules():\n",
    "        if isinstance(value, BottleneckFooling):\n",
    "            for l in (value.conv1_fooling, value.conv2_fooling, value.conv3_fooling):\n",
    "                l.weight.data.zero_()\n",
    "                l.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80328430-844e-48a4-a666-9fd742f6c745",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fooling Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efff4d-a811-4ff4-ac12-e19ea8665816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406]) * 255\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225]) * 255\n",
    "DEFAULT_CROP_RATIO = 224 / 256\n",
    "\n",
    "INCEPTION_MEAN = np.ones(3) * 127\n",
    "INCEPTION_STD = np.ones(3)\n",
    "\n",
    "\n",
    "def create_val_loader(\n",
    "    in_val_dataset: str,\n",
    "    num_workers: int,\n",
    "    batch_size: int,\n",
    "    resolution: int,\n",
    "    distributed: bool,\n",
    "    normalization_mode: Union[Literal[\"imagenet\"], Literal[\"inception\"]] = \"imagenet\",\n",
    ") -> Loader:\n",
    "    \"\"\"Create a dataloader of the ImageNet validation dataset.\n",
    "\n",
    "    Args:\n",
    "        in_val_dataset: The path to the ImageNet validation dataset.\n",
    "        num_workers: The number of workers to use for loading the dataset.\n",
    "        batch_size: The batch size to use for loading the dataset.\n",
    "        resolution: The resolution to load the images at.\n",
    "        distributed: Whether to use distributed training.\n",
    "        normalization_mode: The normalization mode to use for the images.\n",
    "    \"\"\"\n",
    "    this_device = device\n",
    "    val_path = Path(in_val_dataset)\n",
    "    assert val_path.is_file()\n",
    "    res_tuple = (resolution, resolution)\n",
    "    cropper = CenterCropRGBImageDecoder(res_tuple, ratio=DEFAULT_CROP_RATIO)\n",
    "    image_pipeline = [\n",
    "        cropper,\n",
    "        ToTensor(),\n",
    "        ToDevice(torch.device(this_device), non_blocking=True),\n",
    "        ToTorchImage(),\n",
    "    ]\n",
    "\n",
    "    if normalization_mode == \"imagenet\":\n",
    "        image_pipeline.append(NormalizeImage(IMAGENET_MEAN, IMAGENET_STD, np.float16))\n",
    "        print(\"Using imagenet normalization.\")\n",
    "    elif normalization_mode == \"inception\":\n",
    "        image_pipeline.append(NormalizeImage(INCEPTION_MEAN, INCEPTION_STD, np.float16))\n",
    "        print(\"Using inception normalization.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization mode {normalization_mode}\")\n",
    "\n",
    "    label_pipeline = [\n",
    "        IntDecoder(),\n",
    "        ToTensor(),\n",
    "        Squeeze(),\n",
    "        ToDevice(torch.device(this_device), non_blocking=True),\n",
    "    ]\n",
    "\n",
    "    loader = Loader(\n",
    "        in_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        order=OrderOption.SEQUENTIAL,\n",
    "        drop_last=False,\n",
    "        pipelines={\"image\": image_pipeline, \"label\": label_pipeline},\n",
    "        distributed=distributed,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64cabc-fc35-4e70-98c7-9e1b9a359f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(\n",
    "    model: nn.Module, layers: List[str], loader: Loader\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Get intermediate features of a model on natural images.\n",
    "\n",
    "    Args:\n",
    "        model: The model to get features of.\n",
    "        layers: The layers to get features of.\n",
    "        loader: The loader to use for loading images.\n",
    "\n",
    "    Returns:\n",
    "        A list of features for each layer.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    with ModelHook(model, layer_names=layers) as hook:\n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                for x, y in tqdm(loader, leave=False):\n",
    "                    model(x)\n",
    "                    features.append(\n",
    "                        {layer: hook(layer).cpu().numpy() for layer in layers}\n",
    "                    )\n",
    "\n",
    "    features = [np.concatenate([f[layer] for f in features]) for layer in layers]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2547dd0-2159-4bb7-9952-86148a52a9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_maximally_activating_cnn_filter(\n",
    "    model: nn.Module,\n",
    "    upstream_layer_name: str,\n",
    "    downstream_layer_name: str,\n",
    "    downstream_filter_index: int,\n",
    "    target_img: torch.Tensor,\n",
    "    n_steps: int = 10000,\n",
    "    verbose: bool = False,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Find a maximally activating CNN filter.\n",
    "\n",
    "    Args:\n",
    "        model: The model to find the filter for.\n",
    "        upstream_layer_name: The name of the layer to find the filter for.\n",
    "        downstream_layer_name: The name of the layer to find the filter for.\n",
    "        downstream_filter_index: The index of the filter to find the filter for.\n",
    "        img: The target input image.\n",
    "        n_steps: The number of optimization steps to use.\n",
    "        verbose: Whether to print progress.\n",
    "\n",
    "    Returns:\n",
    "        The maximally activating filter, the activation of the filter, and the\n",
    "        bias of the filter.\n",
    "    \"\"\"\n",
    "\n",
    "    assert target_img.ndim == 3\n",
    "    target_img = target_img.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        with render.ModelHook(model) as hook:\n",
    "            model(target_img)\n",
    "            activation = hook(upstream_layer_name)\n",
    "            downstream_activation = hook(downstream_layer_name)[\n",
    "                :, downstream_filter_index\n",
    "            ]\n",
    "\n",
    "    downstream_unit = lucent.modelzoo.util.get_model_layer(model, downstream_layer_name)\n",
    "    unit = duplicate_conv(\n",
    "        downstream_unit, bias=False, overwrite_values={\"out_channels\": 1}\n",
    "    ).to(activation.device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(unit.parameters(), lr=1e-3)\n",
    "    if verbose:\n",
    "        pbar = tqdm(range(n_steps))\n",
    "    else:\n",
    "        pbar = range(n_steps)\n",
    "\n",
    "    for _ in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        out = unit(activation)\n",
    "        channel_activation = out.sum()\n",
    "        loss = -channel_activation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        unit.weight.data = (\n",
    "            unit.weight.data / torch.norm(unit.weight.data.view(-1), keepdim=True)\n",
    "        ).detach()\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_postfix_str(f\"Activation: {channel_activation.item():.4f}\")\n",
    "\n",
    "    return (\n",
    "        unit.weight.data.detach()[0],\n",
    "        downstream_activation,\n",
    "        downstream_unit.bias.data[downstream_filter_index].detach()\n",
    "        if downstream_unit.bias\n",
    "        else torch.zeros((1,), device=target_img.device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2448d-3b91-4ea8-8eb3-d4cec50b94c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def orthogonalize(x: torch.Tensor, base: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Orthogonalize x with respect to base.\n",
    "\n",
    "    Args:\n",
    "        x: The vector to orthogonalize.\n",
    "        base: The base to orthogonalize with respect to.\n",
    "\n",
    "    Returns:\n",
    "        The orthogonalized vector.\n",
    "    \"\"\"\n",
    "    return x - torch.dot(base.flatten(), x.flatten()) * base / torch.dot(\n",
    "        base.flatten(), base.flatten()\n",
    "    )\n",
    "\n",
    "\n",
    "def init_fooling_unit(\n",
    "    model: nn.Module,\n",
    "    layer_name: str,\n",
    "    fooling_layer_name: str,\n",
    "    merge_layer_name: str,\n",
    "    filter_indices: int,\n",
    "    delta_ws: torch.Tensor,\n",
    "    features: torch.Tensor,\n",
    "    lbd: float = 1.0,\n",
    "    natural_activation_levels=None,\n",
    "    weight_factor: float = 1,\n",
    ") -> None:\n",
    "    \"\"\"Initialize a fooling unit.\n",
    "\n",
    "    Args:\n",
    "        model: The model to initialize the fooling unit of.\n",
    "        layer_name: The name of the layer to initialize the fooling unit of.\n",
    "        fooling_layer_name: The name of the fooling layer.\n",
    "        merge_layer_name: The name of the merge layer.\n",
    "        filter_indices: The indices of the filters to initialize.\n",
    "        delta_ws: The delta weights to use for initialization.\n",
    "        features: The features to use for initialization.\n",
    "        lbd: The lambda to use for initialization.\n",
    "        natural_activation_levels: The natural activation levels to use for\n",
    "        initialization.\n",
    "        weight_factor: The weight factor to use for initialization.\n",
    "    \"\"\"\n",
    "    unit = lucent.modelzoo.util.get_model_layer(model, layer_name)\n",
    "    fooling_unit = lucent.modelzoo.util.get_model_layer(model, fooling_layer_name)\n",
    "    merge_unit = lucent.modelzoo.util.get_model_layer(model, merge_layer_name)\n",
    "\n",
    "    bs = []\n",
    "    w_news = []\n",
    "    for idx, filter_index in tqdm(\n",
    "        enumerate(filter_indices), leave=False, desc=\"Setting weights and biases\"\n",
    "    ):\n",
    "        w = unit.weight.data[filter_index]\n",
    "\n",
    "        delta_w = delta_ws[idx]\n",
    "\n",
    "        delta_w = orthogonalize(delta_w, w)\n",
    "        delta_w = delta_w / torch.sum(delta_w**2) * torch.sum(w**2)\n",
    "        assert torch.all(\n",
    "            torch.abs(torch.dot(delta_w.flatten(), w.flatten())) < 1e-4\n",
    "        ), torch.dot(delta_w.flatten(), w.flatten())\n",
    "\n",
    "        w_new = w + lbd * delta_w\n",
    "        w_new = w_new / torch.sum(w_new**2) * torch.sum(w**2)\n",
    "        w_new *= weight_factor\n",
    "\n",
    "        w_news.append(w_new)\n",
    "\n",
    "        if unit.bias is None:\n",
    "            b = torch.zeros((1,), device=w.device)\n",
    "        else:\n",
    "            b = unit.bias.data[filter_index]\n",
    "\n",
    "        bs.append(b)\n",
    "\n",
    "    w_news = torch.stack(w_news, 0)\n",
    "    bs = torch.stack(bs, 0)\n",
    "\n",
    "    if natural_activation_levels is None:\n",
    "        max_natural_activation_levels = None\n",
    "    else:\n",
    "        max_natural_activation_levels = torch.tensor(natural_activation_levels)\n",
    "    batch_size = 128\n",
    "    for start_idx in tqdm(\n",
    "        range(0, len(features), batch_size),\n",
    "        leave=False,\n",
    "        desc=\"Determining necessary bias offset\",\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                inp = torch.tensor(\n",
    "                    features[start_idx : start_idx + batch_size],\n",
    "                    device=device,\n",
    "                    dtype=torch.float16,\n",
    "                )\n",
    "                res = torch.nn.functional.conv2d(inp, w_news)\n",
    "                res = res.max(3)[0].max(2)[0]\n",
    "                if max_natural_activation_levels is None:\n",
    "                    max_natural_activation_levels = res.max(0)[0].cpu()\n",
    "                else:\n",
    "                    max_natural_activation_levels = torch.maximum(\n",
    "                        res.max(0)[0].cpu(), max_natural_activation_levels\n",
    "                    )\n",
    "\n",
    "    delta_bs = -max_natural_activation_levels.to(bs.device)\n",
    "    bs = bs[:, 0]\n",
    "    b_news = bs + delta_bs\n",
    "\n",
    "    fooling_unit.weight.data = w_news.detach()\n",
    "    fooling_unit.bias.data = b_news.detach()\n",
    "\n",
    "    n_filters = fooling_unit.weight.shape[0]\n",
    "\n",
    "    merge_unit.weight.data.zero_()\n",
    "\n",
    "    for filter_index in filter_indices:\n",
    "        merge_unit.weight.data[filter_index, n_filters + filter_index, 1, 1] = 1.0\n",
    "        merge_unit.weight.data[filter_index, filter_index, 1, 1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fd0fd-3ae9-49ed-9c9a-e84e8974a364",
   "metadata": {},
   "source": [
    "# Create ResNet with near-identical FVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a266972-38c1-4662-b57b-c2f9a332eec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "model_fooled = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "replace_module(\n",
    "    model_fooled,\n",
    "    lambda x: isinstance(x, resnet.Bottleneck),\n",
    "    lambda x: BottleneckFooling(x),\n",
    ")\n",
    "reset_fooling_units(model_fooled)\n",
    "\n",
    "missing_keys = model_fooled.load_state_dict(\n",
    "    model.state_dict(), strict=False\n",
    ").missing_keys\n",
    "for k in missing_keys:\n",
    "    if \"_fooling\" not in k:\n",
    "        raise ValueError(f\"Truly missing key: {k}.\")\n",
    "\n",
    "model = model.eval().to(device)\n",
    "model_fooled = model_fooled.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536bb73-5da4-4fa8-9ba3-f5af57ff75c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_loader = create_val_loader(\n",
    "    os.path.expandvars(\"$SCRATCH/datasets/ffcv_imagenet_data/val_500_0.50_90.ffcv\"),\n",
    "    8,\n",
    "    160,\n",
    "    224,\n",
    "    0,\n",
    "    \"imagenet\",\n",
    ")\n",
    "\n",
    "# Either apply this to all residual blocks:\n",
    "# blocks_of_interest = list(filter(lambda x: len(x.split(\"_\")) == 2, lucent.modelzoo.util.get_model_layers(model)))\n",
    "# Or just to a single block:\n",
    "blocks_of_interest = [\"layer4_1\"]\n",
    "\n",
    "pbar = tqdm(blocks_of_interest)\n",
    "for block_of_interest in pbar:\n",
    "    pbar.set_description(f\"{block_of_interest}. Getting features.\")\n",
    "    natural_features = get_features(\n",
    "        model_fooled, [f\"{block_of_interest}_conv1_fooling_merge\"], val_loader\n",
    "    )[0]\n",
    "\n",
    "    # Set up fooling mechanism in layer conv2 of the current residual block.\n",
    "    layer_of_interest = f\"{block_of_interest}_conv2\"\n",
    "\n",
    "    n_units = lucent.modelzoo.util.get_model_layer(\n",
    "        model, layer_of_interest\n",
    "    ).weight.shape[0]\n",
    "\n",
    "    pbar.set_description(f\"{block_of_interest}. Getting target image.\")\n",
    "    target_imgs = render.render_vis(\n",
    "        model_fooled,\n",
    "        f\"{layer_of_interest}:0\",\n",
    "        show_inline=False,\n",
    "        progress=False,\n",
    "        show_image=False,\n",
    "        target_image_shape=(224, 224),\n",
    "        preprocess=\"torchvision\",\n",
    "        params_f=lambda: param.image(224),\n",
    "        thresholds=(2000,),\n",
    "    )\n",
    "    target_img = target_imgs[0][0]\n",
    "    target_img = torch.tensor(np.transpose(target_img, (2, 0, 1))).to(device)\n",
    "\n",
    "    pbar.set_description(f\"{block_of_interest}. Getting weight changes.\")\n",
    "    delta_ws = []\n",
    "    for i in tqdm(range(n_units), leave=False):\n",
    "        delta_w, _, _ = find_maximally_activating_cnn_filter(\n",
    "            model_fooled,\n",
    "            f\"{block_of_interest}_conv1_fooling_merge\",\n",
    "            f\"{block_of_interest}_conv2\",\n",
    "            i,\n",
    "            target_img,\n",
    "            n_steps=50,\n",
    "            verbose=False,\n",
    "        )\n",
    "        delta_ws.append(delta_w)\n",
    "\n",
    "    pbar.set_description(f\"{block_of_interest}. Updating weights.\")\n",
    "\n",
    "    init_fooling_unit(\n",
    "        model_fooled,\n",
    "        layer_of_interest,\n",
    "        f\"{layer_of_interest}_fooling\",\n",
    "        f\"{layer_of_interest}_fooling_merge\",\n",
    "        list(range(len(delta_ws))),\n",
    "        delta_ws,\n",
    "        natural_features,\n",
    "        natural_activation_levels=None,\n",
    "        lbd=3,\n",
    "        weight_factor=100,\n",
    "    )\n",
    "    del natural_features, target_imgs, target_img, delta_w, delta_ws\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68a258-5af3-42a6-ab6d-4ed0522f3d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plot Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0c7d7-bb8e-47b2-a642-996394cc7cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_data = {}\n",
    "\n",
    "for boi in blocks_of_interest:\n",
    "    layer_of_interest = f\"{boi}_conv2\"\n",
    "\n",
    "    if layer_of_interest not in layer_data:\n",
    "        layer_data[layer_of_interest] = ({}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0dd11d-a24c-4929-9bfb-54af9011b3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers_pbar = tqdm(layer_data)\n",
    "for loi in layers_pbar:\n",
    "    layers_pbar.set_description(f\"Layer: {loi}\")\n",
    "    n_units = lucent.modelzoo.util.get_model_layer(model_fooled, loi).weight.shape[0]\n",
    "    if n_units == 512:\n",
    "        units = list(range(0, 513, 50))\n",
    "    elif n_units == 256:\n",
    "        units = list(range(0, 257, 25))\n",
    "    elif n_units == 128:\n",
    "        units = list(range(0, 129, 10))\n",
    "    elif n_units == 64:\n",
    "        units = list(range(0, 65, 6))\n",
    "    else:\n",
    "        raise ValueError(n_units)\n",
    "\n",
    "    vis, fooled_vis = layer_data[loi]\n",
    "\n",
    "    units_pbar = tqdm(units, leave=False)\n",
    "    for i in units_pbar:\n",
    "        units_pbar.set_description(f\"Unit: {i}\")\n",
    "        if i not in fooled_vis:\n",
    "            fooled_vis[i] = render.render_vis(\n",
    "                model_fooled,\n",
    "                f\"{layer_of_interest}_fooling_merge:{i}\",\n",
    "                target_image_shape=(224, 224),\n",
    "                show_inline=False,\n",
    "                progress=False,\n",
    "                params_f=lambda: param.image(224, batch=1),\n",
    "                verbose=False,\n",
    "                thresholds=(512,),\n",
    "                show_image=False,\n",
    "                redirected_activation_warmup=0,\n",
    "                preprocess=\"torchvision\",\n",
    "            )\n",
    "        if i not in vis:\n",
    "            vis[i] = render.render_vis(\n",
    "                model,\n",
    "                f\"{layer_of_interest}:{i}\",\n",
    "                target_image_shape=(224, 224),\n",
    "                show_inline=False,\n",
    "                params_f=lambda: param.image(224, batch=1),\n",
    "                verbose=False,\n",
    "                thresholds=(512,),\n",
    "                progress=False,\n",
    "                show_image=False,\n",
    "                redirected_activation_warmup=0,\n",
    "                preprocess=\"torchvision\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea7930-7c06-42fd-aa4c-db60a4c2a4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in layer_data:\n",
    "    fig, axs = plt.subplots(2, len(vis))\n",
    "    scale = 2\n",
    "    fig.set_size_inches(len(vis) * scale, 2.2 * scale)\n",
    "    for i, unit_id in enumerate(vis):\n",
    "        axs[0, i].axis(\"off\")\n",
    "        axs[1, i].axis(\"off\")\n",
    "\n",
    "        axs[0, i].set_title(f\"Unit {unit_id}\")\n",
    "\n",
    "        axs[0, i].imshow(layer_data[k][0][unit_id][0][0])\n",
    "        axs[1, i].imshow(layer_data[k][1][unit_id][0][0])\n",
    "\n",
    "    fig.text(0.5, 1, f\"Original Network Units ({k}, Conv 2)\", fontsize=14, ha=\"center\")\n",
    "    fig.text(\n",
    "        0.5,\n",
    "        0.025,\n",
    "        f\"Manipulated Network Units ({k}, Conv 2)\",\n",
    "        fontsize=14,\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
