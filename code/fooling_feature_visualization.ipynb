{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ],
      "metadata": {
        "id": "cMmE1CQWPRhI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4upHHBiDS7wt"
      },
      "source": [
        "# Code for \"Don't trust your eyes: on the (un)reliability of feature visualizations\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "pFbgT0bAZE05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jItbFy_dC171"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/zimmerrol/lucent.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psHsfwDiQ7lx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import imageio\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from lucent.optvis.transform import standard_transforms\n",
        "from lucent.optvis import render, param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiedZVGvd65C"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "PROJECT_DIR = '/path/to/project/dir/'\n",
        "CLASSIFIER_WEIGHT_NAME = 'classifier_weights.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definitions"
      ],
      "metadata": {
        "id": "ntaXDp_b2LaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FCaZxajiwww"
      },
      "outputs": [],
      "source": [
        "def convert_rendered_img_to_numpy(img):\n",
        "    x = np.squeeze(img[0]*255.0).astype(np.uint8)\n",
        "    assert np.min(x) >= 0\n",
        "    assert np.max(x) <= 255\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def center_crop(h, w):\n",
        "    def inner(x: torch.Tensor) -> torch.Tensor:\n",
        "        assert len(x.shape) ==4, print(x.shape, type(x))\n",
        "        assert x.shape[2] >= h, print(x.shape, type(x))\n",
        "        assert x.shape[3] >= w, print(x.shape, type(x))\n",
        "\n",
        "        oy = (x.shape[2] - h) // 2\n",
        "        ox = (x.shape[3] - w) // 2\n",
        "\n",
        "        return x[:, :, oy:oy+h, ox:ox+w]\n",
        "\n",
        "    return inner"
      ],
      "metadata": {
        "id": "38wn0ySy36wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(model, idx, show_inline=True, thresholds=(512,), *args, **kwargs):\n",
        "    img_size = 224\n",
        "    img = render.render_vis(model, idx,\n",
        "                           show_inline=show_inline, thresholds=thresholds,\n",
        "                           param_f=lambda: param.image(img_size, batch=1),\n",
        "                           transforms=standard_transforms +\n",
        "                           [center_crop(img_size, img_size)], *args, **kwargs)\n",
        "    return img"
      ],
      "metadata": {
        "id": "CahnFbzbqqe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_multiple_visualizations(dir_path, model, model_name, viz_indices,\n",
        "                                 thresholds=(512,), *args, **kwargs):\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "    for idx in viz_indices:\n",
        "\n",
        "        images = visualize(model, idx, thresholds=thresholds, *args, **kwargs)\n",
        "        assert len(images) == len(thresholds)\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            img_numpy = convert_rendered_img_to_numpy(img)\n",
        "            imageio.imwrite(os.path.join(dir_path, f\"{model_name}_layer-{idx}_threshold-{thresholds[i]}.png\"), img_numpy)"
      ],
      "metadata": {
        "id": "2bRRUg6BbgVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_batch(dir_path=os.path.join(PROJECT_DIR, 'natural-vs-viz-classifier/'),\n",
        "                     n_imgs=12,\n",
        "                     transform=lambda x: x/255.0):\n",
        "\n",
        "    imgs = [[] for _ in range(n_imgs)]\n",
        "    for i in range(len(imgs)):\n",
        "\n",
        "        imgs[i] = Image.open(os.path.join(dir_path, 'test-imgs', f'{(i+1):04d}.png'))\n",
        "        imgs[i] = transform(np.asarray(imgs[i], dtype='float'))\n",
        "\n",
        "    image_batch = np.stack(imgs)\n",
        "    image_batch = np.transpose(image_batch, axes=[0, 3, 1, 2])\n",
        "    print(f\"Loaded {n_imgs} images in batch of shape {image_batch.shape} with min {np.min(image_batch)} and max {np.max(image_batch)}.\")\n",
        "\n",
        "    return image_batch"
      ],
      "metadata": {
        "id": "kZR2Lueu9bWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_classifier_weights(model,\n",
        "                            weight_dir=os.path.join(PROJECT_DIR, 'natural-vs-viz-classifier/'),\n",
        "                            weight_name=CLASSIFIER_WEIGHT_NAME):\n",
        "    \"\"\"Load classifier weights into existing model's weights\"\"\"\n",
        "\n",
        "    weight_path = os.path.join(weight_dir, weight_name)\n",
        "\n",
        "    # preprocess classifier_state_dict\n",
        "    classifier_prelim_state_dict = torch.load(weight_path, map_location=torch.device('cpu'))\n",
        "    classifier_state_dict = {\"classifier.\"+k.replace('module.', ''): v for k, v in classifier_prelim_state_dict.items()}\n",
        "\n",
        "    # remove classifier from original dict\n",
        "    original_state_dict_with_classifier = model.state_dict()\n",
        "    original_state_dict = {k: v for k, v in original_state_dict_with_classifier.items() if \"classifier.\" not in k}\n",
        "\n",
        "    # make sure dicts are different\n",
        "    a = set(original_state_dict.keys())\n",
        "    b = set(classifier_state_dict.keys())\n",
        "    assert len(a.intersection(b)) == 0, f\"matching keys found: {a.intersection(b)}\"\n",
        "\n",
        "    # combine dicts\n",
        "    original_state_dict.update(classifier_state_dict)\n",
        "    print(model.load_state_dict(original_state_dict, strict=False))"
      ],
      "metadata": {
        "id": "QCvR-sAhpNhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manually_set_weights(model, class_offset=100, num_classes=1000,\n",
        "                         orig_last_layer_num_units=1008):\n",
        "    \"\"\"Set weights such that last layer visualizations will be offset by an arbitrary constant.\"\"\"\n",
        "\n",
        "    k = 250.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # weights\n",
        "        layer_1_num_units = 1000\n",
        "        model.lyr_1.weight = torch.nn.Parameter(torch.Tensor(np.zeros([layer_1_num_units,orig_last_layer_num_units])))\n",
        "        for i in range(num_classes):\n",
        "            model.lyr_1.weight[i,i] = 1.0\n",
        "\n",
        "        model.lyr_2.weight = torch.nn.Parameter(torch.Tensor(np.zeros([2000, 1001])))\n",
        "        for i in range(num_classes):\n",
        "            model.lyr_2.weight[i,i] = 1.0\n",
        "            model.lyr_2.weight[i+num_classes, (i + class_offset)%num_classes] = 1.0\n",
        "\n",
        "            model.lyr_2.weight[i, num_classes] = - k\n",
        "            model.lyr_2.weight[i+num_classes, num_classes] = k\n",
        "\n",
        "        model.lyr_3.weight = torch.nn.Parameter(torch.Tensor(np.zeros([1000, 2000])))\n",
        "        for i in range(num_classes):\n",
        "            model.lyr_3.weight[i,i] = 1.0\n",
        "            model.lyr_3.weight[i,i+num_classes] = 1.0\n",
        "\n",
        "        # biases\n",
        "        model.lyr_1.bias = torch.nn.Parameter(torch.Tensor(np.zeros([layer_1_num_units])))\n",
        "        model.lyr_1.bias[0:num_classes] = 100.0\n",
        "\n",
        "        model.lyr_2.bias = torch.nn.Parameter(torch.Tensor(np.zeros([2000])))\n",
        "        model.lyr_2.bias[0:num_classes] = 0.0\n",
        "        model.lyr_2.bias[num_classes:(2*num_classes)] = - k\n",
        "\n",
        "        model.lyr_3.bias = torch.nn.Parameter(torch.Tensor(np.zeros([num_classes])))\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval();\n",
        "    print(f\"Manually setting weights completed with class_offset {class_offset}.\")"
      ],
      "metadata": {
        "id": "SpwLGeO-Vlem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manually_specify_visualization_weights(model, target_img_idx=0, input_size=224):\n",
        "    \"\"\"Set weights such that last layer visualizations will show a target image.\"\"\"\n",
        "\n",
        "    image_batch = load_image_batch()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        class_offset = 100\n",
        "        num_classes = 1000\n",
        "        k = 250\n",
        "\n",
        "        # weights\n",
        "        layer_1_num_units = 1000\n",
        "        model.lyr_1.weight = torch.nn.Parameter(torch.Tensor(np.zeros([layer_1_num_units, 1008])))\n",
        "        for i in range(num_classes):\n",
        "            model.lyr_1.weight[i,i] = 1.0\n",
        "\n",
        "        model.lyr_2.weight = torch.nn.Parameter(torch.Tensor(np.zeros([2000, 1002])))\n",
        "        for i in range(num_classes):\n",
        "            model.lyr_2.weight[i,i] = 1.0\n",
        "            model.lyr_2.weight[i+num_classes, 1001] = 1.0\n",
        "\n",
        "            model.lyr_2.weight[i, num_classes] = - k\n",
        "            model.lyr_2.weight[i+num_classes, num_classes] = k\n",
        "\n",
        "        model.lyr_3.weight = torch.nn.Parameter(torch.Tensor(np.zeros([1000, 2000])))\n",
        "        for i in range(num_classes):\n",
        "            model.lyr_3.weight[i,i] = 1.0\n",
        "            model.lyr_3.weight[i,i+num_classes] = 1.0\n",
        "\n",
        "        # biases\n",
        "        model.lyr_1.bias = torch.nn.Parameter(torch.Tensor(np.zeros([layer_1_num_units])))\n",
        "        model.lyr_1.bias[0:num_classes] = 100.0\n",
        "\n",
        "        model.lyr_2.bias = torch.nn.Parameter(torch.Tensor(np.zeros([2000])))\n",
        "        model.lyr_2.bias[0:num_classes] = 0.0\n",
        "        model.lyr_2.bias[num_classes:(2*num_classes)] = - k\n",
        "\n",
        "        model.lyr_3.bias = torch.nn.Parameter(torch.Tensor(np.zeros([num_classes])))\n",
        "\n",
        "        model.viz_layer.weight = torch.nn.Parameter(torch.Tensor(np.zeros([1, 3, 224, 224]) - 0.1))\n",
        "        for c in range(3):\n",
        "            for i in range(input_size):\n",
        "                for j in range(input_size):\n",
        "                    # dividing by input_size**2 ensures that the layer activations don't explode during visualization\n",
        "                    model.viz_layer.weight[:,c,i,j] = image_batch[target_img_idx,c,i,j]/(input_size**2)\n",
        "\n",
        "    model.to(device).eval();"
      ],
      "metadata": {
        "id": "c-MFDuh9_sli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Figure: visualization-trajectory"
      ],
      "metadata": {
        "id": "R2wzKmTe6w6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lucent.modelzoo.inceptionv1.InceptionV1 import InceptionV1 as INCEPTION_V1\n",
        "model = INCEPTION_V1(pretrained=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval();"
      ],
      "metadata": {
        "id": "Btz7xx_v6xDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = os.path.join(PROJECT_DIR, 'visualization-trajectory/')"
      ],
      "metadata": {
        "id": "IkgzjDpH6xJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_multiple_visualizations(dir_path=dir_path,\n",
        "                             model=model,\n",
        "                             model_name=\"inception-v1-unmodified\",\n",
        "                             viz_indices=[f\"softmax2_pre_activation_matmul:0\" for x in [0]],\n",
        "                             thresholds=(1, 2, 4, 8, 16, 32, 64, 128, 256, 512))"
      ],
      "metadata": {
        "id": "cptJcxmd6xMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure: permuted-visualizations-offset-100"
      ],
      "metadata": {
        "id": "cpCJJE3NZ_RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lucent.modelzoo.inceptionv1.InceptionV3 import InceptionV3 as INCEPTION_V3\n",
        "model = INCEPTION_V3(pretrained=True, add_custom_layers=True,\n",
        "                     use_RELU_in_custom_layers=True, verbose=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval();\n",
        "assert type(model.classifier) is not None"
      ],
      "metadata": {
        "id": "Ewo0ftjbZ_XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_classifier_weights(model)"
      ],
      "metadata": {
        "id": "Jy57j6Iea6qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manually_set_weights(model=model, class_offset=100, num_classes=1000)"
      ],
      "metadata": {
        "id": "DqEgePp1bBLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = os.path.join(PROJECT_DIR, 'permuted-visualizations-offset-100/')"
      ],
      "metadata": {
        "id": "Whtf4dafb_kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_multiple_visualizations(dir_path=dir_path,\n",
        "                             model=model,\n",
        "                             model_name=\"inception-v3-offset-100\",\n",
        "                             viz_indices=[f\"lyr_3:{x}\" for x in range(0, 1000, 100)])"
      ],
      "metadata": {
        "id": "uZCS1vPpaLFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure: original-visualizations"
      ],
      "metadata": {
        "id": "wohSaxToM3KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lucent.modelzoo.inceptionv1.InceptionV1 import InceptionV1 as INCEPTION_V1\n",
        "model = INCEPTION_V1(pretrained=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval();"
      ],
      "metadata": {
        "id": "O_iraySMM4Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = os.path.join(PROJECT_DIR, 'original-visualizations/')"
      ],
      "metadata": {
        "id": "fgN6NO7rNFhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_multiple_visualizations(dir_path=dir_path,\n",
        "                             model=model,\n",
        "                             model_name=\"inception-v1\",\n",
        "                             viz_indices=[f\"softmax2_pre_activation_matmul:{x}\" for x in range(0, 1000, 100)])"
      ],
      "metadata": {
        "id": "6KzozqhuM3S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure: manually-specified-visualizations"
      ],
      "metadata": {
        "id": "Ngf3Vxy_7fHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lucent.modelzoo.inceptionv1.InceptionV4 import InceptionV4 as INCEPTION_V4\n",
        "model = INCEPTION_V4(pretrained=True, add_custom_layers=True,\n",
        "                     use_RELU_in_custom_layers=True, verbose=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval();\n",
        "assert type(model.classifier) is not None"
      ],
      "metadata": {
        "id": "MqMcGGCj_gWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_classifier_weights(model)"
      ],
      "metadata": {
        "id": "CO3PfQ5MpD_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manually_specify_visualization_weights(model, target_img_idx=7)"
      ],
      "metadata": {
        "id": "fElQUVTAFJ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure classifier predicts natural images for natural images\n",
        "_ = model(torch.Tensor(load_image_batch(transform=lambda x: x-117.0)).cuda())"
      ],
      "metadata": {
        "id": "t3mOixkl7lCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = os.path.join(PROJECT_DIR, 'manually-specified-visualizations/')"
      ],
      "metadata": {
        "id": "QZ3jN5xnSt5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_imgs = 12\n",
        "for i in range(num_imgs):\n",
        "    manually_specify_visualization_weights(model, target_img_idx=i)\n",
        "    save_multiple_visualizations(dir_path=dir_path,\n",
        "                                 model=model,\n",
        "                                 model_name=f\"inception-v4-img-{i}\",\n",
        "                                 viz_indices=[\"lyr_3:0\"],\n",
        "                                 thresholds=(2,4,6,8,10,12,14,16,20,32))"
      ],
      "metadata": {
        "id": "6Az-DK-3FXQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure: silent-units"
      ],
      "metadata": {
        "id": "xAozFiNJVsZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "from lucent.modelzoo import resnet50\n",
        "model = resnet50(pretrained=True)\n",
        "model.to(device).eval();"
      ],
      "metadata": {
        "id": "BCS7KcbsVshK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = os.path.join(PROJECT_DIR, 'silent-units-layer3-1-conv3/'"
      ],
      "metadata": {
        "id": "aMyroaKdSxUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# silent units\n",
        "for layer in [\"layer3_1_conv3\", \"layer3_1_bn3\"]:\n",
        "    save_multiple_visualizations(dir_path=dir_path,\n",
        "                                 model=model,\n",
        "                                 model_name=\"ResNet-50\",\n",
        "                                 viz_indices=[f\"{layer}:{idx}\" for idx in [147, 316, 342, 405, 750]],\n",
        "                                 thresholds=(512,))"
      ],
      "metadata": {
        "id": "L-7RwUX09LF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# non-silent units\n",
        "for layer in [\"layer3_1_conv3\", \"layer3_1_bn3\"]:\n",
        "    save_multiple_visualizations(dir_path=dir_path,\n",
        "                                    model=model,\n",
        "                                    model_name=\"ResNet-50\",\n",
        "                                    viz_indices=[f\"{layer}:{idx}\" for idx in [172, 184, 324, 581, 968]],\n",
        "                                    thresholds=(512,))"
      ],
      "metadata": {
        "id": "qf98q-3THdy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}